{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import splitfolders\n",
    "import joblib\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: split-folders in /Users/mysterious/opt/anaconda3/envs/tensor_lecture/lib/python3.7/site-packages (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3747 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        'Data_Split/train',\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=3747,\n",
    "        color_mode='grayscale',\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 936 images belonging to 2 classes.\n",
      "Found 1173 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = val_datagen.flow_from_directory('Data_Split/val',\n",
    "                                                        target_size=(150, 150),\n",
    "                                                        batch_size=936,\n",
    "                                                        color_mode='grayscale',\n",
    "                                                        class_mode='binary')\n",
    "test_generator = test_datagen.flow_from_directory('Data_Split/test',\n",
    "                                                  target_size=(150, 150),\n",
    "                                                  batch_size=1173,\n",
    "                                                  color_mode='grayscale',\n",
    "                                                  class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3747 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Creating the augumented data\n",
    "\n",
    "aug_train_images = ImageDataGenerator(rotation_range=30, \n",
    "                                   width_shift_range=0.25, \n",
    "                                   height_shift_range=0.25, \n",
    "                                   shear_range=0.25, \n",
    "                                   zoom_range=0.25, \n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True)\n",
    "\n",
    "train_aug = aug_train_images.flow_from_directory('Data_Split/train',\n",
    "                                                  target_size=(150, 150),\n",
    "                                                  batch_size=3747,\n",
    "                                                  color_mode='grayscale',\n",
    "                                                  class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting images and labels for models\n",
    "train_data, train_labels = next (train_generator)\n",
    "test_data, test_labels = next (test_generator)\n",
    "val_data, val_labels = next (validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping\n",
    "train_data = train_data.reshape(train_data.shape[0], -1)\n",
    "test_data = test_data.reshape(test_data.shape[0], -1)\n",
    "val_data = val_data.reshape(val_data.shape[0], -1)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save model results into a pickle file\n",
    "def pickle_model(model):\n",
    "    \n",
    "    file_name = input(\"What would you like to name the model?\")\n",
    "    file_name = file_name.replace(' ', '')\n",
    "    # use the built-in open() function to open a file\n",
    "    output_file = open(f\"./models/{file_name}.pkl\", \"wb\") # \"wb\" means \"write as bytes\"\n",
    "    # dump the variable's contents into the file\n",
    "    joblib.dump(model, output_file)\n",
    "    # close the file, ensuring nothing stays in the buffer\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = next (train_generator)\n",
    "test_data, test_labels = next (test_generator)\n",
    "val_data, val_labels = next (validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 147, 147, 64)      1088      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 73, 73, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 71, 71, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 35, 35, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 39200)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                627216    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 646,785\n",
      "Trainable params: 646,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn3_model = models.Sequential()\n",
    "cnn3_model.add(layers.Conv2D(64, (4, 4), activation='relu',\n",
    "                      input_shape=(150, 150, 1)))\n",
    "cnn3_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn3_model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn3_model.add(layers.MaxPooling2D((2,2)))\n",
    "cnn3_model.add(layers.Flatten())\n",
    "cnn3_model.add(layers.Dense(16, activation='relu'))\n",
    "cnn3_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn3_model.compile(optimizer=\"adam\",\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy', metrics.Precision(), metrics.Recall()])\n",
    "\n",
    "cnn3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe7ebcdd710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe7ebcdd710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.3789 - accuracy: 0.8284 - precision: 0.8242 - recall: 0.9722WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe7f3273830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe7f3273830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "118/118 [==============================] - 142s 1s/step - loss: 0.3789 - accuracy: 0.8284 - precision: 0.8242 - recall: 0.9722 - val_loss: 0.2732 - val_accuracy: 0.9199 - val_precision: 0.9222 - val_recall: 0.9722\n",
      "Epoch 2/10\n",
      "118/118 [==============================] - 143s 1s/step - loss: 0.2754 - accuracy: 0.9242 - precision: 0.9277 - recall: 0.9718 - val_loss: 0.2488 - val_accuracy: 0.9370 - val_precision: 0.9432 - val_recall: 0.9722\n",
      "Epoch 3/10\n",
      "118/118 [==============================] - 132s 1s/step - loss: 0.2568 - accuracy: 0.9314 - precision: 0.9447 - recall: 0.9623 - val_loss: 0.2300 - val_accuracy: 0.9380 - val_precision: 0.9395 - val_recall: 0.9780\n",
      "Epoch 4/10\n",
      "118/118 [==============================] - 139s 1s/step - loss: 0.2446 - accuracy: 0.9367 - precision: 0.9473 - recall: 0.9671 - val_loss: 0.2500 - val_accuracy: 0.9370 - val_precision: 0.9937 - val_recall: 0.9195\n",
      "Epoch 5/10\n",
      "118/118 [==============================] - 180s 2s/step - loss: 0.2219 - accuracy: 0.9458 - precision: 0.9583 - recall: 0.9678 - val_loss: 0.2071 - val_accuracy: 0.9434 - val_precision: 0.9437 - val_recall: 0.9810\n",
      "Epoch 6/10\n",
      "118/118 [==============================] - 207s 2s/step - loss: 0.1980 - accuracy: 0.9557 - precision: 0.9659 - recall: 0.9737 - val_loss: 0.1935 - val_accuracy: 0.9573 - val_precision: 0.9599 - val_recall: 0.9824\n",
      "Epoch 7/10\n",
      "118/118 [==============================] - 170s 1s/step - loss: 0.1939 - accuracy: 0.9536 - precision: 0.9668 - recall: 0.9696 - val_loss: 0.1991 - val_accuracy: 0.9412 - val_precision: 0.9337 - val_recall: 0.9898\n",
      "Epoch 8/10\n",
      "118/118 [==============================] - 172s 1s/step - loss: 0.1782 - accuracy: 0.9586 - precision: 0.9691 - recall: 0.9744 - val_loss: 0.1863 - val_accuracy: 0.9455 - val_precision: 0.9802 - val_recall: 0.9444\n",
      "Epoch 9/10\n",
      "118/118 [==============================] - 168s 1s/step - loss: 0.1697 - accuracy: 0.9626 - precision: 0.9758 - recall: 0.9729 - val_loss: 0.2016 - val_accuracy: 0.9402 - val_precision: 0.9324 - val_recall: 0.9898\n",
      "Epoch 10/10\n",
      "118/118 [==============================] - 209s 2s/step - loss: 0.1601 - accuracy: 0.9592 - precision: 0.9698 - recall: 0.9744 - val_loss: 0.1759 - val_accuracy: 0.9519 - val_precision: 0.9691 - val_recall: 0.9649\n"
     ]
    }
   ],
   "source": [
    "cnn3_history = cnn3_model.fit(train_data,\n",
    "                                            train_labels,\n",
    "                                            batch_size=32,\n",
    "                                            epochs=10,\n",
    "                                            validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like to name the model?CNN3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7fe787486170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7fe787486170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fe7876c8680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fe7876c8680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://acead4aa-416a-49c0-a6d1-08bca198c422/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://acead4aa-416a-49c0-a6d1-08bca198c422/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pickle_model(cnn3_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cnn3_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_lecture",
   "language": "python",
   "name": "tensor_lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
